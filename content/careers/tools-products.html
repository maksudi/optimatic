---
title: Tools and products
---

<p>Adaptic is not aligned to any particular vendor, however, these are the tools and products we commonly recommend and utilise at our clients.</p>

<h3>Cluster computing</h3>

<p>Map-reduce on Apache Hadoop is the industry-standard for scaling distributed computations, but, it is a break-through which is still evolving.</p> 
    
<p><a href="https://spark.apache.org/">Apache Spark</a> is growing in profile and for good reason. With distributed computation primitives which are more powerful and flexible than simple map-reduce, and advanced dataset management which makes better use of the cluster's RAM, Spark is a fantastic match for iterative machine learning algorithms, as well as the batch and stream-oriented workloads required when we use the <a href="http://www.manning.com/marz/">Lambda Architecture</a>. The Spark API leverages your Python knowledge, but is also accessible from Scala or Java. Spark leverages many distributed SQL and NoSQL formats and data stores in the cluster and communes happily on top of the Hadoop YARN and HDFS infrastructures, including in AWS or other cloud.</p>

<p>In most enterprise data hub architectures there is a place for a high-performance relational database or database appliance in the serving layer. On AWS, this requirement can be met very effectively by <a href="https://aws.amazon.com/redshift/">Redshift</a>, but we are equally comfortable working with <a href="http://www.ibm.com/ibm/puresystems/us/en/pd_analytics.html">Netezza<a>, <a href="http://www.pivotal.io/big-data/pivotal-greenplum-database">Greenplum</a> or <a href="http://www.oracle.com/us/solutions/ent-performance-bi/business-intelligence/exalytics-bi-machine/overview/index.html">Exalytics</a>. 

<h3>Analytics and machine learning</h3>

<p><a href="https://spark.apache.org/mllib/">MLlib</a> (and <a href="https://mahout.apache.org/users/sparkbindings/home.html">soon Mahout</a>) are excellent platforms for scaling machine learning algorithms. These make data- and processing-intensive algorithms feasible to apply to full customer data sets with hundreds or thousands of features.</p>

<p>However, for smaller problems and prototyping bigger ones we utilise other tools including the awesome open source <a href="http://www.r-project.org/">R</a>, <a href="http://www.gnu.org/software/octave/">Octave</a> and <a href="http://community.rapidminer.com/">RapidMiner</a> applications as well as the Python <a href="http://scikit-learn.org/stable/">scikit-learn</a> ecosystem.</p>

<p>These applications and platforms offer the building blocks for our key machine learning techniques, which include:</p>
<ul>
    <li>Linear/logistic regression</li>
    <li>Support vector machines</li>
    <li>Neural networks</li>
    <li>Meta-heuristics (including genetic algorithms)</li>
    <li>Discrete event simulators</li>
    <li>Collaborative filtering</li>
    <li>Ensemble methods</li>
</ul>

<h3>Presentation and visualisation</h3>

<p>We are absolute believers in the importance of exploring and communicating the insight available from data and models and modern visualisation tools (which transcend report-writers) are the main method to empower business domain experts, analysts and data scientists to influence decisions. <a href="http://www.tableausoftware.com/">Tableau</a> and <a href="http://www.qlik.com/">QlikView</a> are two examples we are happy to use.</p>

<p>For discovery analytics, an appliance which we can use to quickly ingest, categorise, navigate and present large amounts of structured and unstructured data is <a href="http://www.sas.com/en_us/software/business-intelligence/visual-analytics.html">SAS Visual Analytics</a> which we have found worth consideration and useful in certain circumstances.</p>  

<p>From the mining industry, we have experience designing and implementing dashboards and control systems in <a href="http://www.ventyx.com/en/solutions/business-operations/business-products/focalpoint">FocalPoint Suite</a>.</p>

<h3>Methods and modelling</h3>  

<p>We are proponents of agile methods and favour modelling and design standards which facilitate early delivery of working software and resilience under change and iteration.</p>
 
<p>Specifically, for tightly governed data assets we recommend <a href="http://danlinstedt.com/">Data Vault</a> modelling (a variant of anchor modelling) and intermediate representation for analytics called Entity Analytic Record (popularised by <a href="http://www.ambiata.com">Ambiata</a>).</p>

<p>For serving and presentation, maximising clear semantics and reuse in all kinds of reporting tools, we have not found a replacement for the conventional <a href="https://en.wikipedia.org/wiki/Star_schema">Star schema (Kimball)</a>.</p>